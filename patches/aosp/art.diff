diff --git a/compiler/jni/quick/jni_compiler.cc b/compiler/jni/quick/jni_compiler.cc
index 036cdbb0cc..af828dfda9 100644
--- a/compiler/jni/quick/jni_compiler.cc
+++ b/compiler/jni/quick/jni_compiler.cc
@@ -518,6 +518,7 @@ static JniCompiledMethod ArtJniCompileMethodInternal(const CompilerOptions& comp
   }
 
   // 9. Plant call to native code associated with method.
+  // XXX jni call here
   MemberOffset jni_entrypoint_offset =
       ArtMethod::EntryPointFromJniOffset(InstructionSetPointerSize(instruction_set));
   if (UNLIKELY(is_critical_native)) {
diff --git a/compiler/optimizing/code_generator_arm64.cc b/compiler/optimizing/code_generator_arm64.cc
index fd5f689c9d..433c93e343 100644
--- a/compiler/optimizing/code_generator_arm64.cc
+++ b/compiler/optimizing/code_generator_arm64.cc
@@ -1128,7 +1128,11 @@ void CodeGeneratorARM64::MaybeIncrementHotness(bool is_frame_entry) {
       __ Ldr(lr, MemOperand(tr, entrypoint_offset));
       // Note: we don't record the call here (and therefore don't generate a stack
       // map), as the entrypoint should never be suspended.
-      __ Blr(lr);
+      if (HasPAC()) {
+        __ Blraaz(lr);
+      } else {
+        __ Blr(lr);
+      }
       if (HasEmptyFrame()) {
         CHECK(is_frame_entry);
         __ Ldr(lr, MemOperand(sp, 8));
@@ -1784,6 +1788,9 @@ void CodeGeneratorARM64::InvokeRuntime(QuickEntrypointEnum entrypoint,
   // For JIT, thunk sharing is per-method, so the gains would be smaller or even negative.
   if (slow_path == nullptr || GetCompilerOptions().IsJitCompiler()) {
     __ Ldr(lr, MemOperand(tr, entrypoint_offset.Int32Value()));
+    if (HasPAC()) {
+      __ Xpaclri();
+    }
     // Ensure the pc position is recorded immediately after the `blr` instruction.
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
     __ blr(lr);
@@ -1805,7 +1812,11 @@ void CodeGeneratorARM64::InvokeRuntimeWithoutRecordingPcInfo(int32_t entry_point
                                                              SlowPathCode* slow_path) {
   ValidateInvokeRuntimeWithoutRecordingPcInfo(instruction, slow_path);
   __ Ldr(lr, MemOperand(tr, entry_point_offset));
-  __ Blr(lr);
+  if (HasPAC()) {
+    __ Blraaz(lr);
+  } else {
+    __ Blr(lr);
+  }
 }
 
 void InstructionCodeGeneratorARM64::GenerateClassInitializationCheck(SlowPathCodeARM64* slow_path,
@@ -4263,6 +4274,7 @@ void InstructionCodeGeneratorARM64::VisitInvokeInterface(HInvokeInterface* invok
   __ Ldr(temp, MemOperand(temp, method_offset));
   // lr = temp->GetEntryPoint();
   __ Ldr(lr, MemOperand(temp, entry_point.Int32Value()));
+  __ Xpaclri();
 
   {
     // Ensure the pc position is recorded immediately after the `blr` instruction.
@@ -4390,6 +4402,7 @@ void CodeGeneratorARM64::GenerateStaticOrDirectCall(
       __ Ldr(lr, MemOperand(
           XRegisterFrom(callee_method),
           ArtMethod::EntryPointFromQuickCompiledCodeOffset(kArm64PointerSize).Int32Value()));
+      __ xpaclri();
       {
         // Use a scope to help guarantee that `RecordPcInfo()` records the correct pc.
         ExactAssemblyScope eas(GetVIXLAssembler(),
@@ -4443,6 +4456,7 @@ void CodeGeneratorARM64::GenerateVirtualCall(
   __ Ldr(temp, MemOperand(temp, method_offset));
   // lr = temp->GetEntryPoint();
   __ Ldr(lr, MemOperand(temp, entry_point.SizeValue()));
+  __ Xpaclri();
   {
     // Use a scope to help guarantee that `RecordPcInfo()` records the correct pc.
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
@@ -6555,6 +6569,9 @@ static void LoadReadBarrierMarkIntrospectionEntrypoint(arm64::Arm64Assembler& as
   const int32_t entry_point_offset =
       Thread::ReadBarrierMarkEntryPointsOffset<kArm64PointerSize>(ip0.GetCode());
   __ Ldr(entrypoint, MemOperand(tr, entry_point_offset));
+  if (HasPAC()) {
+    __ Autiza(entrypoint);
+  }
 }
 
 void CodeGeneratorARM64::CompileBakerReadBarrierThunk(Arm64Assembler& assembler,
@@ -6610,7 +6627,7 @@ void CodeGeneratorARM64::CompileBakerReadBarrierThunk(Arm64Assembler& assembler,
         __ Ldar(ip0.W(), MemOperand(base_reg));
       }
       // Do not unpoison. With heap poisoning enabled, the entrypoint expects a poisoned reference.
-      __ Br(ip1);                           // Jump to the entrypoint.
+      __ Br(ip1);
       break;
     }
     case BakerReadBarrierKind::kArray: {
@@ -6635,7 +6652,7 @@ void CodeGeneratorARM64::CompileBakerReadBarrierThunk(Arm64Assembler& assembler,
       __ Bfi(ip1, ip0, 3, 6);               // Insert ip0 to the entrypoint address to create
                                             // a switch case target based on the index register.
       __ Mov(ip0, base_reg);                // Move the base register to ip0.
-      __ Br(ip1);                           // Jump to the entrypoint's array switch case.
+      __ Br(ip1);
       break;
     }
     case BakerReadBarrierKind::kGcRoot: {
diff --git a/compiler/utils/arm64/assembler_arm64.cc b/compiler/utils/arm64/assembler_arm64.cc
index d722e00646..a073f70e08 100644
--- a/compiler/utils/arm64/assembler_arm64.cc
+++ b/compiler/utils/arm64/assembler_arm64.cc
@@ -54,6 +54,9 @@ static void SetVIXLCPUFeaturesFromART(vixl::aarch64::MacroAssembler* vixl_masm_,
   if (art_features->HasLSE()) {
     features->Combine(vixl::CPUFeatures::kAtomics);
   }
+  if (HasPAC()) {
+    features->Combine(vixl::CPUFeatures::kPAuth);
+  }
 }
 
 Arm64Assembler::Arm64Assembler(ArenaAllocator* allocator,
@@ -101,6 +104,9 @@ void Arm64Assembler::JumpTo(ManagedRegister m_base, Offset offs, ManagedRegister
   UseScratchRegisterScope temps(&vixl_masm_);
   temps.Exclude(reg_x(base.AsXRegister()), reg_x(scratch.AsXRegister()));
   ___ Ldr(reg_x(scratch.AsXRegister()), MEM_OP(reg_x(base.AsXRegister()), offs.Int32Value()));
+  if (HasPAC()) {
+    ___ Xpaci(reg_x(scratch.AsXRegister()));
+  }
   ___ Br(reg_x(scratch.AsXRegister()));
 }
 
diff --git a/compiler/utils/arm64/assembler_arm64.h b/compiler/utils/arm64/assembler_arm64.h
index fe2f1766c2..5fe4890f80 100644
--- a/compiler/utils/arm64/assembler_arm64.h
+++ b/compiler/utils/arm64/assembler_arm64.h
@@ -43,6 +43,17 @@ class Arm64InstructionSetFeatures;
 
 namespace arm64 {
 
+inline bool HasPAC() {
+#if !defined(__aarch64__)
+  static bool is_pac = getenv("TARGET_EXPERIMENTAL_PAC");
+  return is_pac;
+#elif defined(ANDROID_EXPERIMENTAL_PAC)
+  return true;
+#else
+  return false;
+#endif
+}
+
 static inline dwarf::Reg DWARFReg(vixl::aarch64::CPURegister reg) {
   if (reg.IsFPRegister()) {
     return dwarf::Reg::Arm64Fp(reg.GetCode());
diff --git a/compiler/utils/arm64/jni_macro_assembler_arm64.cc b/compiler/utils/arm64/jni_macro_assembler_arm64.cc
index bb93a96ebe..f6d1e3433b 100644
--- a/compiler/utils/arm64/jni_macro_assembler_arm64.cc
+++ b/compiler/utils/arm64/jni_macro_assembler_arm64.cc
@@ -629,20 +629,31 @@ void Arm64JNIMacroAssembler::Jump(ManagedRegister m_base, Offset offs) {
   UseScratchRegisterScope temps(asm_.GetVIXLAssembler());
   Register scratch = temps.AcquireX();
   ___ Ldr(scratch, MEM_OP(reg_x(base.AsXRegister()), offs.Int32Value()));
-  ___ Br(scratch);
+  if (HasPAC()) {
+    ___ Braaz(scratch);
+  } else {
+    ___ Br(scratch);
+  }
 }
 
 void Arm64JNIMacroAssembler::Call(ManagedRegister m_base, Offset offs) {
   Arm64ManagedRegister base = m_base.AsArm64();
   CHECK(base.IsXRegister()) << base;
   ___ Ldr(lr, MEM_OP(reg_x(base.AsXRegister()), offs.Int32Value()));
-  ___ Blr(lr);
+  if (HasPAC()) {
+    ___ Blraaz(lr);
+  } else {
+    ___ Blr(lr);
+  }
 }
 
 void Arm64JNIMacroAssembler::Call(FrameOffset base, Offset offs) {
   // Call *(*(SP + base) + offset)
   ___ Ldr(lr, MEM_OP(reg_x(SP), base.Int32Value()));
   ___ Ldr(lr, MEM_OP(lr, offs.Int32Value()));
+  if (HasPAC()) {
+    ___ Xpaclri();
+  }
   ___ Blr(lr);
 }
 
@@ -773,7 +784,11 @@ void Arm64JNIMacroAssembler::EmitExceptionPoll(Arm64Exception* exception) {
           MEM_OP(reg_x(TR),
                  QUICK_ENTRYPOINT_OFFSET(kArm64PointerSize, pDeliverException).Int32Value()));
 
-  ___ Blr(lr);
+  if (HasPAC()) {
+    ___ Blraaz(lr);
+  } else {
+    ___ Blr(lr);
+  }
   // Call should never return.
   ___ Brk();
 }
diff --git a/libartbase/base/mem_map.cc b/libartbase/base/mem_map.cc
index bd880dacb6..7c92c2a3fe 100644
--- a/libartbase/base/mem_map.cc
+++ b/libartbase/base/mem_map.cc
@@ -134,7 +134,7 @@ uintptr_t CreateStartPos(uint64_t input) {
 #endif
 
 static uintptr_t GenerateNextMemPos() {
-#ifdef __BIONIC__
+#if 0
   uint64_t random_data;
   arc4random_buf(&random_data, sizeof(random_data));
   return CreateStartPos(random_data);
diff --git a/runtime/arch/arm64/fault_handler_arm64.cc b/runtime/arch/arm64/fault_handler_arm64.cc
index c139e21d05..b0126243a4 100644
--- a/runtime/arch/arm64/fault_handler_arm64.cc
+++ b/runtime/arch/arm64/fault_handler_arm64.cc
@@ -74,6 +74,13 @@ void FaultManager::GetMethodAndReturnPcAndSp(siginfo_t* siginfo ATTRIBUTE_UNUSED
   *out_return_pc = sc->pc + 4;
 }
 
+template <typename T> T *MaybeStripPointer(T *ptr) {
+#ifdef ANDROID_EXPERIMENTAL_PAC
+  ptr = __builtin_ptrauth_strip(ptr, 0);
+#endif
+  return ptr;
+}
+
 bool NullPointerHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info, void* context) {
   if (!IsValidImplicitCheck(info)) {
     return false;
@@ -90,7 +97,8 @@ bool NullPointerHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info, void*
   *reinterpret_cast<uintptr_t*>(sc->sp) = sc->pc + 4;
   sc->regs[30] = reinterpret_cast<uintptr_t>(info->si_addr);
 
-  sc->pc = reinterpret_cast<uintptr_t>(art_quick_throw_null_pointer_exception_from_signal);
+  sc->pc = reinterpret_cast<uintptr_t>(
+      MaybeStripPointer(art_quick_throw_null_pointer_exception_from_signal));
   VLOG(signals) << "Generating null pointer exception";
   return true;
 }
@@ -145,7 +153,8 @@ bool SuspensionHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info ATTRIBU
     // ldr x0,[x0,#0] instruction (r0 will be 0, set by the trigger).
 
     sc->regs[30] = sc->pc + 4;
-    sc->pc = reinterpret_cast<uintptr_t>(art_quick_implicit_suspend);
+    sc->pc = reinterpret_cast<uintptr_t>(
+        MaybeStripPointer(art_quick_implicit_suspend));
 
     // Now remove the suspend trigger that caused this fault.
     Thread::Current()->RemoveSuspendTrigger();
@@ -184,7 +193,8 @@ bool StackOverflowHandler::Action(int sig ATTRIBUTE_UNUSED, siginfo_t* info ATTR
   // The value of LR must be the same as it was when we entered the code that
   // caused this fault.  This will be inserted into a callee save frame by
   // the function to which this handler returns (art_quick_throw_stack_overflow).
-  sc->pc = reinterpret_cast<uintptr_t>(art_quick_throw_stack_overflow);
+  sc->pc = reinterpret_cast<uintptr_t>(
+      MaybeStripPointer(art_quick_throw_stack_overflow));
 
   // The kernel will now return to the address in sc->pc.
   return true;
diff --git a/runtime/arch/arm64/jni_entrypoints_arm64.S b/runtime/arch/arm64/jni_entrypoints_arm64.S
index 8a34662645..553271f412 100644
--- a/runtime/arch/arm64/jni_entrypoints_arm64.S
+++ b/runtime/arch/arm64/jni_entrypoints_arm64.S
@@ -86,7 +86,12 @@ ENTRY art_jni_dlsym_lookup_stub
     .cfi_restore x30
 
     cbz   x17, 1f   // is method code null ?
+    // XXX jni call here
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    braaz x17       // if non-null, tail call to method's code.
+#else
     br    x17       // if non-null, tail call to method's code.
+#endif
 
 1:
     ret             // restore regs and return to caller to handle exception.
diff --git a/runtime/arch/arm64/quick_entrypoints_arm64.S b/runtime/arch/arm64/quick_entrypoints_arm64.S
index 634c762040..af70d6e12f 100644
--- a/runtime/arch/arm64/quick_entrypoints_arm64.S
+++ b/runtime/arch/arm64/quick_entrypoints_arm64.S
@@ -342,7 +342,13 @@ NO_ARG_RUNTIME_EXCEPTION art_quick_throw_stack_overflow, artThrowStackOverflowFr
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
     cbz    x0, 1f                         // did we find the target? if not go to exception delivery
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    // braaz  xIP0                           // tail call to target
+    xpaci xIP0
     br     xIP0                           // tail call to target
+#else
+    br     xIP0                           // tail call to target
+#endif
 1:
     DELIVER_PENDING_EXCEPTION
 .endm
@@ -405,6 +411,9 @@ SAVE_SIZE=6*8   // x4, x5, x19, x20, FP, LR saved.
     // load method-> METHOD_QUICK_CODE_OFFSET
     ldr x9, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
     // Branch to method.
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    xpaci x9
+#endif
     blr x9
 
     // Pop the ArtMethod* (null), arguments and alignment padding from the stack.
@@ -878,6 +887,10 @@ ENTRY art_quick_do_long_jump
 
     REFRESH_MARKING_REGISTER
 
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    // this can go to art_quick_to_interpreter_bridge
+    xpaci xIP1
+#endif
     br  xIP1
 END art_quick_do_long_jump
 
@@ -1744,6 +1757,9 @@ ENTRY art_quick_imt_conflict_trampoline
     // and jump to it.
     ldr x0, [xIP1, #__SIZEOF_POINTER__]
     ldr xIP0, [x0, #ART_METHOD_QUICK_CODE_OFFSET_64]
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    xpaci xIP0
+#endif
     br xIP0
 .Lconflict_trampoline:
     // Call the runtime stub to populate the ImtConflictTable and jump to the
@@ -1800,6 +1816,9 @@ ENTRY art_quick_resolution_trampoline
     ldr x0, [sp, #0]        // artQuickResolutionTrampoline puts called method in *SP.
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    xpaci xIP0 // check this
+#endif
     br xIP0
 1:
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
@@ -1916,7 +1935,12 @@ ENTRY art_quick_generic_jni_trampoline
     // Apply the new SP for out args, releasing unneeded reserved area.
     mov sp, xIP1
 
+    // XXX jni call here
+#ifdef ANDROID_EXPERIMENTAL_PAC
+    blraaz xIP0     // native call.
+#else
     blr xIP0        // native call.
+#endif
 
     // result sign extension is handled in C code
     // prepare for artQuickGenericJniEndTrampoline call
diff --git a/runtime/gc/space/image_space.cc b/runtime/gc/space/image_space.cc
index 5484f49e96..5e82d95637 100644
--- a/runtime/gc/space/image_space.cc
+++ b/runtime/gc/space/image_space.cc
@@ -112,7 +112,7 @@ static int32_t ChooseRelocationOffsetDelta(int32_t min_delta, int32_t max_delta)
   return r;
 }
 
-static int32_t ChooseRelocationOffsetDelta() {
+__attribute__((unused)) static int32_t ChooseRelocationOffsetDelta() {
   return ChooseRelocationOffsetDelta(ART_BASE_ADDRESS_MIN_DELTA, ART_BASE_ADDRESS_MAX_DELTA);
 }
 
@@ -174,7 +174,7 @@ static bool GenerateImage(const std::string& image_filename,
   CHECK_EQ(image_isa, kRuntimeISA)
       << "We should always be generating an image for the current isa.";
 
-  int32_t base_offset = ChooseRelocationOffsetDelta();
+  int32_t base_offset = ART_BASE_ADDRESS_MIN_DELTA;
   LOG(INFO) << "Using an offset of 0x" << std::hex << base_offset << " from default "
             << "art base address of 0x" << std::hex << ART_BASE_ADDRESS;
   arg_vector.push_back(StringPrintf("--base=0x%x", ART_BASE_ADDRESS + base_offset));
@@ -2581,7 +2581,7 @@ class ImageSpace::BootImageLoader {
 
     // Reserve address space. If relocating, choose a random address for ALSR.
     uint8_t* addr = reinterpret_cast<uint8_t*>(
-        relocate_ ? ART_BASE_ADDRESS + ChooseRelocationOffsetDelta() : base_address);
+        relocate_ ? ART_BASE_ADDRESS + ART_BASE_ADDRESS_MIN_DELTA : base_address);
     MemMap image_reservation =
         ReserveBootImageMemory(addr, image_reservation_size + extra_reservation_size, error_msg);
     if (!image_reservation.IsValid()) {
